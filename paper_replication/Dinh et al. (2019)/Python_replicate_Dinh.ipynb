{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dinh et al. (2019)\n",
    "\n",
    "## A data-driven approach to predicting diabetes and cardiovascular disease with machine learning\n",
    "\n",
    "URL: https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-019-0918-5\n",
    "\n",
    "\n",
    "## Brief Summary\n",
    "\n",
    "Dinh et al. (2019) uses different ML models (logistic regression, support vector machines, random forest, and gradient boosting) on NHANES dataset to predict i) Diabetes and ii) Cardiovascular disease (\"CVD\").\n",
    "\n",
    "**Goal**: Identification mechanism for patients at risk of diabetes and cardiovascular diseases and key contributors to diabetes .\n",
    "\n",
    "**Results**:\n",
    "\n",
    "Best scores:\n",
    "\n",
    "- CVB prediction based on 131 NHANES variables achieved an AU-ROC score of 83.9% .\n",
    "- Diabetes prediction based on 123 NHANES variables achieved an AU-ROC score of 95.7% .\n",
    "- Pre-diabetic prediction based on 123 NHANES variables achieved an AU-ROC score of 84.4% .\n",
    "- Top 5 predictors in diabetes patients were 1) `waist size`, 2) `age`, 3) `self-reported weight`, 4) `leg length`, 5) `sodium intake`.\n",
    "\n",
    "\n",
    "\n",
    "This notebook replicates the results of the paper. The structure follows the following steps: \n",
    "\n",
    "1. NHANES data \n",
    "2. Pre-processing of the data\n",
    "3. Transformation of the data\n",
    "4. Train/Test Split \n",
    "5. CV 10-fold\n",
    "6. Training monitoring using MLflow\n",
    "7. Get metric results (AUC)\n",
    "\n",
    "\n",
    "The structure of the analysis emulates the Figure 1 from the paper: \n",
    "\n",
    "![Fig 1 from Dinh et al. 2019](https://raw.githubusercontent.com/pipegalera/ml_diabetes/main/images/dinh_2019_Fig1.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 4208\n",
    "DATA_PATH = \"/root/dev/ml_diabetes/data/raw_data/NHANES/\"\n",
    "DINH_DOCS_PATH = \"/root/dev/ml_diabetes/data/processed/NHANES/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "## 1. HNANES data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariates and Targets \n",
    "\n",
    "- Source: https://www.cdc.gov/nchs/index.htm\n",
    "- Downloaded raw data via: `notebooksnhanes_data_backfill`\n",
    "\n",
    "\n",
    "The paper did not mention what variables they use from NHANES. I emailed the author in the correspondence section of the paper to try to get the list of variables they used, but no answer from him yet.\n",
    "\n",
    "Please notice that NHANES have more than 3900 variables, therefore without the list of the specific variables used it is impossible to fully replicate the paper.\n",
    "\n",
    "For now, I will consider the variables taken from [Figure 5](https://raw.githubusercontent.com/pipegalera/ml_diabetes/main/images/dinh_2019_Fig5.png) and [Figure 6](https://raw.githubusercontent.com/pipegalera/ml_diabetes/main/images/dinh_2019_Fig6.png) of the paper. I compiled them by hand in an Excel file using NHANES search tool for variables (see: `processed/NHANES/dinh_2019_variables_doc.xlsx`).\n",
    "\n",
    "\n",
    "- `Case I: Diabetes`\n",
    "\n",
    "    - Glucose >= 126 mg/dL. OR;\n",
    "    - 1 to the question \"Have you ever been told by a doctor that you have diabetes?\"\n",
    "\n",
    "- `Case II: Undiagnosed Diabetes`\n",
    "\n",
    "    - Glucose >= 126 mg/dL. AND;\n",
    "    - \"No\" to the question \"Have you ever been told by a doctor that you have diabetes?\"\n",
    "\n",
    "- `Cardiovascular disease`\n",
    "\n",
    "    - 1 to any of the the questions \"Have you ever been told by a doctor that you had congestive heart failure, coronary heart disease, a heart attack, or a stroke?\"\n",
    "\n",
    "- `Pre diabetes`\n",
    "\n",
    "    - Glucose 125 >= 100 mg/dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable Name</th>\n",
       "      <th>NHANES Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Age</td>\n",
       "      <td>RIDAGEYR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alcohol consumption</td>\n",
       "      <td>ALQ130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alcohol intake</td>\n",
       "      <td>DRXTALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alcohol intake, First Day</td>\n",
       "      <td>DR1TALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alcohol intake, Second Day</td>\n",
       "      <td>DR2TALCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Arm circumference</td>\n",
       "      <td>BMXARMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arm length</td>\n",
       "      <td>BMXARML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Blood osmolality</td>\n",
       "      <td>LBXSOSSI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Blood relatives have diabetes</td>\n",
       "      <td>MCQ250A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Blood urea nitrogen</td>\n",
       "      <td>LBDSBUSI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BMI</td>\n",
       "      <td>BMXBMI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Caffeine intake</td>\n",
       "      <td>DRXTCAFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Caffeine intake, First Day</td>\n",
       "      <td>DR1TCAFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Caffeine intake, Second Day</td>\n",
       "      <td>DR2TCAFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Calcium intake, First Day</td>\n",
       "      <td>DR1TCALC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Variable Name NHANES Name\n",
       "0                             Age    RIDAGEYR\n",
       "1             Alcohol consumption      ALQ130\n",
       "2                  Alcohol intake    DRXTALCO\n",
       "3       Alcohol intake, First Day    DR1TALCO\n",
       "4      Alcohol intake, Second Day    DR2TALCO\n",
       "5               Arm circumference     BMXARMC\n",
       "6                      Arm length     BMXARML\n",
       "7                Blood osmolality    LBXSOSSI\n",
       "8   Blood relatives have diabetes     MCQ250A\n",
       "9             Blood urea nitrogen    LBDSBUSI\n",
       "10                            BMI      BMXBMI\n",
       "11                Caffeine intake    DRXTCAFF\n",
       "12     Caffeine intake, First Day    DR1TCAFF\n",
       "13    Caffeine intake, Second Day    DR2TCAFF\n",
       "14      Calcium intake, First Day    DR1TCALC"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinh_2019_vars = pd.read_excel(f\"{DINH_DOCS_PATH}dinh_2019_variables_doc.xlsx\")\n",
    "\n",
    "dinh_2019_vars[[\"Variable Name\", \"NHANES Name\"]].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the complete list of variables, check the file `dinh_2019_variables_doc.xlsx` under NHANES data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "NHANES data is made by multiple files (see `NHANES` unde data folder) that have to be compiled together. The data was downloaded automatically via script, all the files converted from SAS to parquet, and the files were stacked and merged based on the individual index (\"SEQN\"). For more details please check the `nhanes_data_backfill` notebook. \n",
    "\n",
    "Plese notice that no transformation are made to the covariates, the files were only arranged and stacked together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(f\"{DATA_PATH}dinh_raw_data.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>RIDAGEYR</th>\n",
       "      <th>ALQ130</th>\n",
       "      <th>DRXTALCO</th>\n",
       "      <th>DR1TALCO</th>\n",
       "      <th>DR2TALCO</th>\n",
       "      <th>BMXARMC</th>\n",
       "      <th>BMXARML</th>\n",
       "      <th>LBXSOSSI</th>\n",
       "      <th>...</th>\n",
       "      <th>SEQ060</th>\n",
       "      <th>DIQ010</th>\n",
       "      <th>MCQ160B</th>\n",
       "      <th>MCQ160b</th>\n",
       "      <th>MCQ160C</th>\n",
       "      <th>MCQ160c</th>\n",
       "      <th>MCQ160E</th>\n",
       "      <th>MCQ160e</th>\n",
       "      <th>MCQ160F</th>\n",
       "      <th>MCQ160f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.2</td>\n",
       "      <td>18.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.8</td>\n",
       "      <td>38.2</td>\n",
       "      <td>288.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.7</td>\n",
       "      <td>25.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.4</td>\n",
       "      <td>20.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.8</td>\n",
       "      <td>39.7</td>\n",
       "      <td>276.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SEQN       YEAR  RIDAGEYR  ALQ130  DRXTALCO  DR1TALCO  DR2TALCO  BMXARMC  \\\n",
       "0   1.0  1999-2000       2.0     NaN      0.00       NaN       NaN     15.2   \n",
       "1   2.0  1999-2000      77.0     1.0      0.00       NaN       NaN     29.8   \n",
       "2   3.0  1999-2000      10.0     NaN      0.00       NaN       NaN     19.7   \n",
       "3   4.0  1999-2000       1.0     NaN      0.00       NaN       NaN     16.4   \n",
       "4   5.0  1999-2000      49.0     3.0     34.56       NaN       NaN     35.8   \n",
       "\n",
       "   BMXARML  LBXSOSSI  ...  SEQ060  DIQ010  MCQ160B  MCQ160b  MCQ160C  MCQ160c  \\\n",
       "0     18.6       NaN  ...     NaN     2.0      NaN      NaN      NaN      NaN   \n",
       "1     38.2     288.0  ...     NaN     2.0      2.0      NaN      2.0      NaN   \n",
       "2     25.5       NaN  ...     NaN     2.0      NaN      NaN      NaN      NaN   \n",
       "3     20.4       NaN  ...     NaN     2.0      NaN      NaN      NaN      NaN   \n",
       "4     39.7     276.0  ...     NaN     2.0      2.0      NaN      2.0      NaN   \n",
       "\n",
       "   MCQ160E  MCQ160e  MCQ160F  MCQ160f  \n",
       "0      NaN      NaN      NaN      NaN  \n",
       "1      2.0      NaN      2.0      NaN  \n",
       "2      NaN      NaN      NaN      NaN  \n",
       "3      NaN      NaN      NaN      NaN  \n",
       "4      2.0      NaN      2.0      NaN  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82091"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SEQN', 'YEAR', 'RIDAGEYR', 'ALQ130', 'DRXTALCO', 'DR1TALCO',\n",
       "       'DR2TALCO', 'BMXARMC', 'BMXARML', 'LBXSOSSI', 'MCQ250A', 'LBDSBUSI',\n",
       "       'BMXBMI', 'DRXTCAFF', 'DR1TCAFF', 'DR2TCAFF', 'DR1TCALC', 'DR2TCALC',\n",
       "       'DRXTCALC', 'DR1TCARB', 'DR2TCARB', 'DRXTCARB', 'LBXSNASI', 'LBXSCLSI',\n",
       "       'MCQ300c', 'MCQ300C', 'BPXDI1', 'BPXDI4', 'BPXDI2', 'BPXDI3',\n",
       "       'RIDRETH1', 'DR1TFIBE', 'DR2TFIBE', 'DRXTFIBE', 'LBXSGTSI', 'HSD010',\n",
       "       'HUQ010', 'LBDHDLSI', 'LBDHDDSI', 'BMXHT', 'BPQ080', 'INDHHIN2',\n",
       "       'DRXTKCAL', 'DR1TKCAL', 'DR2TKCAL', 'LBDLDLSI', 'BMXLEG', 'LBDLYMNO',\n",
       "       'LBXMCVSI', 'BPXPLS', 'WHD140', 'DR1TSODI', 'DR2TSODI', 'DRDTSODI',\n",
       "       'BPXSY1', 'BPXSY4', 'BPXSY2', 'BPXSY3', 'LBDTCSI', 'LBDSTRSI',\n",
       "       'BMXWAIST', 'BMXWT', 'LBXWBCSI', 'LBXSASSI', 'LBXGLUSI', 'LBDGLUSI',\n",
       "       'RHQ141', 'RHD143', 'SEQ060', 'DIQ010', 'MCQ160B', 'MCQ160b', 'MCQ160C',\n",
       "       'MCQ160c', 'MCQ160E', 'MCQ160e', 'MCQ160F', 'MCQ160f'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "## 2. Pre-processing and Data modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "\n",
    "### 2.1 Extreme values and replacing Missing/Don't know answers\n",
    "\n",
    "> The preprocessing stage also converted any undecipherable values (errors in datatypes and standard formatting) from the database to null representations.\n",
    "\n",
    "For this, I've checked the variables according to their possible values in the NHANES documentation (https://wwwn.cdc.gov/nchs/nhanes/search/default.aspx). I did not found any any extreme value out of the possible ranges. However, the data is reviwed and updated after the survey, so it might be that the NCHS applied some fixes after they saw them. \n",
    "\n",
    "\n",
    "I have replaced \"Don't know\" and \"Refused\" for NA values and converted the intial encoding of the categorical variables to the real values in the survey - given that the encoding is not consistent accross years. For the model, I will encode the variables myself so I don't have to jungle NHANES encoding. \n",
    "\n",
    "All the variables can by found at  https://wwwn.cdc.gov/nchs/nhanes/search/default.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are numerical variables that have coded \"Don't know\" and \"Refused\" as numerical\n",
    "df['ALQ130'] = df['ALQ130'].replace([77, 99, 777, 999], np.nan)\n",
    "df['WHD140'] = df['WHD140'].replace([7777, 77777, 9999, 99999], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2 Homogenize variables that are the same but are called diffrent in different NHANES years\n",
    "\n",
    "Intake variables went from 1 day in 1999 to 2001 to 2 days from 2003 on, therefore the variable has to be homogenized. Dinh et al. (2019) do not specify which examination records the authors, but my best guess is that they problably took the average of both days that the examination was performed. \n",
    "\n",
    "This situation happends with:\n",
    "\n",
    "- Alcohol intake (`DRXTALCO`, `DR1TALCO`, `DR2TALCO`)\n",
    "- Caffeine intake (`DRXTCAFF`, `DR1TCAFF`, `DR2TCAFF`)\n",
    "- Calcium intake (`DRXTCALC`, `DR1TCALC`, `DR2TCALC`)\n",
    "- Carbohydrate intake (`DRXTCARB`, `DR1TCARB`, `DR2TCARB`)\n",
    "- Fiber intake (`DRXTFIBE`, `DR1TFIBE`, `DR2TFIBE`)\n",
    "- Kcal intake (`DRXTKCAL`, `DR1TKCAL`, `DR2TKCAL`)\n",
    "- Sodium intake (`DRDTSODI`, `DR1TSODI`, `DR2TSODI`)\n",
    "\n",
    "\n",
    "Also, small changes in same quesion format are registered with different codes. Examples: \n",
    "\n",
    "- `MCQ250A`, `MCQ300C` and `MCQ300c`\n",
    "- `LBDHDDSI` and `LBDHDLSI`\n",
    "- `LBXGLUSI` and `LBDGLUSI`\n",
    "- `SEQ060`,`RHQ141`, and `RHD143`\n",
    "\n",
    "And same questions are coded differnetly as well:\n",
    "\n",
    "- `MCQ160B` and `MCQ160b`\n",
    "- `MCQ160C` and `MCQ160c`\n",
    "- `MCQ160E` and `MCQ160e`\n",
    "- `MCQ160F` and `MCQ160F`\n",
    "\n",
    "\n",
    "It can be seen here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar questions (or the same) with different NHANES variable codes\n",
    "var_docs = pd.read_excel(f\"{DINH_DOCS_PATH}dinh_2019_variables_doc.xlsx\")\n",
    "\n",
    "filtered_var_docs = var_docs[var_docs['NHANES Name'].isin(\n",
    "    ['MCQ250A', 'MCQ300C', 'MCQ300c', \n",
    "     'LBDHDDSI', 'LBDHDLSI', 'LBXGLUSI', 'LBDGLUSI']\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix that, I will create a function that creates an average of the Intake variable of Day 1 and Day and average them, givin only one variable - for example \"Alcohol_Intake\" instead of having 'DRXTALCO', 'DR1TALCO', 'DR2TALCO'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_intake_new_column(df, day0_col, day1_col, day2_col):\n",
    "    return np.where(df[day0_col].isna(),\n",
    "                    df[[day1_col, day2_col]].mean(axis=1, skipna=True),\n",
    "                    df[day0_col])\n",
    "\n",
    "# Assuming df is already defined and loaded with data\n",
    "\n",
    "# Create new columns\n",
    "df['Alcohol_Intake'] = create_intake_new_column(df, 'DRXTALCO', 'DR1TALCO', 'DR2TALCO')\n",
    "df['Caffeine_Intake'] = create_intake_new_column(df, 'DRXTCAFF', 'DR1TCAFF', 'DR2TCAFF')\n",
    "df['Calcium_Intake'] = create_intake_new_column(df, 'DRXTCALC', 'DR1TCALC', 'DR2TCALC')\n",
    "df['Carbohydrate_Intake'] = create_intake_new_column(df, 'DRXTCARB', 'DR1TCARB', 'DR2TCARB')\n",
    "df['Fiber_Intake'] = create_intake_new_column(df, 'DRXTFIBE', 'DR1TFIBE', 'DR2TFIBE')\n",
    "df['Kcal_Intake'] = create_intake_new_column(df, 'DRXTKCAL', 'DR1TKCAL', 'DR2TKCAL')\n",
    "df['Sodium_Intake'] = create_intake_new_column(df, 'DRDTSODI', 'DR1TSODI', 'DR2TSODI')\n",
    "\n",
    "# Coalesce functions\n",
    "df['Relative_Had_Diabetes'] = df['MCQ250A'].combine_first(df['MCQ300C']).combine_first(df['MCQ300c'])\n",
    "df['Told_CHF'] = df['MCQ160B'].combine_first(df['MCQ160b'])\n",
    "df['Told_CHD'] = df['MCQ160C'].combine_first(df['MCQ160c'])\n",
    "df['Told_HA'] = df['MCQ160E'].combine_first(df['MCQ160e'])\n",
    "df['Told_stroke'] = df['MCQ160F'].combine_first(df['MCQ160f'])\n",
    "df['Pregnant'] = df['SEQ060'].combine_first(df['RHQ141']).combine_first(df['RHD143'])\n",
    "df['HDL_Cholesterol'] = df['LBDHDLSI'].combine_first(df['LBDHDDSI'])\n",
    "df['Glucose'] = df['LBXGLUSI'].combine_first(df['LBDGLUSI'])\n",
    "\n",
    "# Delete old columns that are not needed\n",
    "columns_to_drop = ['DRXTALCO', 'DR1TALCO', 'DR2TALCO', 'DRXTCAFF', 'DR1TCAFF', 'DR2TCAFF',\n",
    "                   'DRXTCALC', 'DR1TCALC', 'DR2TCALC', 'DRXTCARB', 'DR1TCARB', 'DR2TCARB',\n",
    "                   'DRXTFIBE', 'DR1TFIBE', 'DR2TFIBE', 'DRXTKCAL', 'DR1TKCAL', 'DR2TKCAL',\n",
    "                   'DRDTSODI', 'DR1TSODI', 'DR2TSODI', 'MCQ250A', 'MCQ300C', 'MCQ300c',\n",
    "                   'MCQ160B', 'MCQ160b', 'MCQ160C', 'MCQ160c', 'MCQ160E', 'MCQ160e',\n",
    "                   'MCQ160F', 'MCQ160f', 'SEQ060', 'RHQ141', 'RHD143',\n",
    "                   'LBDHDLSI', 'LBDHDDSI', 'LBXGLUSI', 'LBDGLUSI']\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1999-2000', '2001-2002', '2003-2004', '2005-2006', '2007-2008',\n",
       "       '2009-2010', '2011-2012', '2013-2014'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Relative_Had_Diabetes'].isna()]['YEAR'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Choosing between different readings in Blood analysis \n",
    "\n",
    "[From NHANES](https://wwwn.cdc.gov/Nchs/Nhanes/2013-2014/BPX_H.htm): \n",
    "\n",
    "> After resting quietly in a seated position for 5 minutes and once the participants maximum inflation level (MIL) has been determined, three consecutive blood pressure readings are obtained. If a blood pressure measurement is interrupted or incomplete, a fourth attempt may be made. All BP determinations (systolic and diastolic) are taken in the mobile examination center (MEC). \n",
    "\n",
    "In Dinh et al. (2019) the authors do not say which readings are taking, but I'm assuming they take the last one to avoid the [white coat syndrom](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5352963/) and for data consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns\n",
    "df['Diastolic_Blood_Pressure'] = df['BPXDI4'].combine_first(df['BPXDI3']).combine_first(df['BPXDI2']).combine_first(df['BPXDI1'])\n",
    "df['Systolic_Blood_Pressure'] = df['BPXSY4'].combine_first(df['BPXSY3']).combine_first(df['BPXSY2']).combine_first(df['BPXSY1'])\n",
    "\n",
    "# Delete old columns that are not needed\n",
    "columns_to_drop = ['BPXDI4', 'BPXDI3', 'BPXDI2', 'BPXDI1',\n",
    "                   'BPXSY4', 'BPXSY3', 'BPXSY2', 'BPXSY1']\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Discretional trimming of the data according to the authors\n",
    "\n",
    "> In our study, all datasets were limited to non-pregnant subjects and adults of at least twenty years of age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter out pregnant individuals and those under 20 years old\n",
    "cond_1 = (df['Pregnant'].isna()) | (df['Pregnant'] != 1)\n",
    "cond_2 =(df['RIDAGEYR'] >= 20)\n",
    "df = df[cond_1 & cond_2]\n",
    "\n",
    "#df = df.drop(columns=['Pregnant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42655"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "### 2.5 Creating the  Target Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "Tables 1 & 3 from Dinh et al. 2019:\n",
    "\n",
    "From [Tables 1 & 3 from Dinh et al. 2019](https://raw.githubusercontent.com/pipegalera/ml_diabetes/main/images/dinh_2019_Table1_3.png):\n",
    "\n",
    "`Diabetes = 1` if\n",
    "\n",
    "- Glucose >= 126 mg/dL. OR;\n",
    "- \"Yes\" to the question \"Have you ever been told by a doctor that you have diabetes?\"\n",
    "\n",
    "`undiagnosed diabetes = 1` if\n",
    "\n",
    "- Glucose >= 126 mg/dL. AND;\n",
    "- \"No\" to the question \"Have you ever been told by a doctor that you have diabetes?\" and had a blood glucose level greater than or equal\n",
    "\n",
    "`pre diabetes = 1` if\n",
    "\n",
    "- Glucose 125 >= 100 mg/dL\n",
    "\n",
    "`CVD = 1` if\n",
    "\n",
    "- \"Yes\" to any of the the questions \"Have you ever been told by a doctor that you had congestive heart failure, coronary heart disease, a heart attack, or a stroke?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  3.,  9., nan,  7.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['DIQ010'].unique() #1=yes, 2=no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Diabetes_Case_I'] = np.where(\n",
    "  (df['Glucose'] > 7.0) | (df['DIQ010'] == 1), \n",
    "  1, \n",
    "  0)\n",
    "\n",
    "\n",
    "df['Diabetes_Case_II'] = np.where(\n",
    "  (df['Diabetes_Case_I'] == 0) & (df['Glucose'] >= 5.6) & (df['Glucose'] < 7.0), \n",
    "  1, \n",
    "  0)\n",
    "\n",
    "df['CVD'] = np.where(\n",
    "    (df['Told_CHF'] == 1) | (df['Told_CHD'] == 1) | (df['Told_HA'] == 1) | (df['Told_stroke'] == 1),\n",
    "    1, \n",
    "    0)\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "#df = df.drop(columns=['Told_CHF', 'Told_CHD', 'Told_HA', 'Told_stroke', 'Glucose', 'DIQ010'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "### 2.6 Column name formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'ALQ130': 'Alcohol_consumption',\n",
    "    'BMXARMC': 'Arm_circumference',\n",
    "    'BMXARML': 'Arm_length',\n",
    "    'BMXBMI': 'Body_mass_index',\n",
    "    'BMXHT': 'Height',\n",
    "    'BMXLEG': 'Leg_length',\n",
    "    'BMXWAIST': 'Waist_circumference',\n",
    "    'BMXWT': 'Weight',\n",
    "    'BPQ080': 'Told_High_Cholesterol',\n",
    "    'BPXPLS': 'Pulse',\n",
    "    'HSD010': 'General_health',\n",
    "    'HUQ010': 'Health_status',\n",
    "    'INDHHIN2': 'Household_income',\n",
    "    'LBXSCLSI': 'Chloride',\n",
    "    'LBXSNASI': 'Sodium',\n",
    "    'LBDLDLSI': 'LDL_cholesterol',\n",
    "    'LBDLYMNO': 'Lymphocytes',\n",
    "    'LBDSBUSI': 'Blood_urea_nitrogen',\n",
    "    'LBDSTRSI': 'Triglycerides',\n",
    "    'LBDTCSI': 'Total_cholesterol',\n",
    "    'LBXMCVSI': 'Mean_cell_volume',\n",
    "    'LBXSASSI': 'Aspartate_aminotransferase_AST',\n",
    "    'LBXSGTSI': 'Gamma_glutamyl_transferase',\n",
    "    'LBXSOSSI': 'Osmolality',\n",
    "    'LBXWBCSI': 'White_blood_cell_count',\n",
    "    'RIDAGEYR': 'Age',\n",
    "    'RIDRETH1': 'Race_ethnicity',\n",
    "    'WHD140': 'Self_reported_greatest_weight',\n",
    "    'YEAR': 'Survey_year'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SEQN', 'Survey_year', 'Age', 'Alcohol_consumption',\n",
       "       'Arm_circumference', 'Arm_length', 'Osmolality', 'Blood_urea_nitrogen',\n",
       "       'Body_mass_index', 'Sodium', 'Chloride', 'Race_ethnicity',\n",
       "       'Gamma_glutamyl_transferase', 'General_health', 'Health_status',\n",
       "       'Height', 'Told_High_Cholesterol', 'Household_income',\n",
       "       'LDL_cholesterol', 'Leg_length', 'Lymphocytes', 'Mean_cell_volume',\n",
       "       'Pulse', 'Self_reported_greatest_weight', 'Total_cholesterol',\n",
       "       'Triglycerides', 'Waist_circumference', 'Weight',\n",
       "       'White_blood_cell_count', 'Aspartate_aminotransferase_AST', 'DIQ010',\n",
       "       'Alcohol_Intake', 'Caffeine_Intake', 'Calcium_Intake',\n",
       "       'Carbohydrate_Intake', 'Fiber_Intake', 'Kcal_Intake', 'Sodium_Intake',\n",
       "       'Relative_Had_Diabetes', 'Told_CHF', 'Told_CHD', 'Told_HA',\n",
       "       'Told_stroke', 'Pregnant', 'HDL_Cholesterol', 'Glucose',\n",
       "       'Diastolic_Blood_Pressure', 'Systolic_Blood_Pressure',\n",
       "       'Diabetes_Case_I', 'Diabetes_Case_II', 'CVD'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEQN</th>\n",
       "      <th>Survey_year</th>\n",
       "      <th>Age</th>\n",
       "      <th>Alcohol_consumption</th>\n",
       "      <th>Arm_circumference</th>\n",
       "      <th>Arm_length</th>\n",
       "      <th>Osmolality</th>\n",
       "      <th>Blood_urea_nitrogen</th>\n",
       "      <th>Body_mass_index</th>\n",
       "      <th>Sodium</th>\n",
       "      <th>...</th>\n",
       "      <th>Told_HA</th>\n",
       "      <th>Told_stroke</th>\n",
       "      <th>Pregnant</th>\n",
       "      <th>HDL_Cholesterol</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>Diastolic_Blood_Pressure</th>\n",
       "      <th>Systolic_Blood_Pressure</th>\n",
       "      <th>Diabetes_Case_I</th>\n",
       "      <th>Diabetes_Case_II</th>\n",
       "      <th>CVD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>77.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.8</td>\n",
       "      <td>38.2</td>\n",
       "      <td>288.0</td>\n",
       "      <td>6.80</td>\n",
       "      <td>24.90</td>\n",
       "      <td>144.1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.39</td>\n",
       "      <td>4.646</td>\n",
       "      <td>56.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>35.8</td>\n",
       "      <td>39.7</td>\n",
       "      <td>276.0</td>\n",
       "      <td>5.70</td>\n",
       "      <td>29.10</td>\n",
       "      <td>137.5</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.08</td>\n",
       "      <td>5.550</td>\n",
       "      <td>82.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.7</td>\n",
       "      <td>38.1</td>\n",
       "      <td>283.0</td>\n",
       "      <td>3.60</td>\n",
       "      <td>29.39</td>\n",
       "      <td>143.2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>4.756</td>\n",
       "      <td>82.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>43.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>4.60</td>\n",
       "      <td>30.94</td>\n",
       "      <td>140.9</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.31</td>\n",
       "      <td>4.989</td>\n",
       "      <td>96.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.0</td>\n",
       "      <td>1999-2000</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>37.2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>30.62</td>\n",
       "      <td>141.3</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.98</td>\n",
       "      <td>4.606</td>\n",
       "      <td>100.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82082</th>\n",
       "      <td>83723.0</td>\n",
       "      <td>2013-2014</td>\n",
       "      <td>61.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.8</td>\n",
       "      <td>40.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>6.07</td>\n",
       "      <td>33.10</td>\n",
       "      <td>138.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.27</td>\n",
       "      <td>8.826</td>\n",
       "      <td>68.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82083</th>\n",
       "      <td>83724.0</td>\n",
       "      <td>2013-2014</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>9.28</td>\n",
       "      <td>24.90</td>\n",
       "      <td>141.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82085</th>\n",
       "      <td>83726.0</td>\n",
       "      <td>2013-2014</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82086</th>\n",
       "      <td>83727.0</td>\n",
       "      <td>2013-2014</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.9</td>\n",
       "      <td>35.2</td>\n",
       "      <td>285.0</td>\n",
       "      <td>4.64</td>\n",
       "      <td>24.50</td>\n",
       "      <td>143.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.42</td>\n",
       "      <td>5.995</td>\n",
       "      <td>76.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82088</th>\n",
       "      <td>83729.0</td>\n",
       "      <td>2013-2014</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.6</td>\n",
       "      <td>277.0</td>\n",
       "      <td>3.57</td>\n",
       "      <td>34.00</td>\n",
       "      <td>139.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42655 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          SEQN Survey_year   Age  Alcohol_consumption  Arm_circumference  \\\n",
       "1          2.0   1999-2000  77.0                  1.0               29.8   \n",
       "4          5.0   1999-2000  49.0                  3.0               35.8   \n",
       "6          7.0   1999-2000  59.0                  NaN               31.7   \n",
       "9         10.0   1999-2000  43.0                  1.0               37.6   \n",
       "11        12.0   1999-2000  37.0                  3.0               37.2   \n",
       "...        ...         ...   ...                  ...                ...   \n",
       "82082  83723.0   2013-2014  61.0                  2.0               35.8   \n",
       "82083  83724.0   2013-2014  80.0                  NaN               31.0   \n",
       "82085  83726.0   2013-2014  40.0                  NaN               31.0   \n",
       "82086  83727.0   2013-2014  26.0                  3.0               29.9   \n",
       "82088  83729.0   2013-2014  42.0                  NaN               37.0   \n",
       "\n",
       "       Arm_length  Osmolality  Blood_urea_nitrogen  Body_mass_index  Sodium  \\\n",
       "1            38.2       288.0                 6.80            24.90   144.1   \n",
       "4            39.7       276.0                 5.70            29.10   137.5   \n",
       "6            38.1       283.0                 3.60            29.39   143.2   \n",
       "9            43.0       281.0                 4.60            30.94   140.9   \n",
       "11           40.0       283.0                 7.10            30.62   141.3   \n",
       "...           ...         ...                  ...              ...     ...   \n",
       "82082        40.0       280.0                 6.07            33.10   138.0   \n",
       "82083        36.0       287.0                 9.28            24.90   141.0   \n",
       "82085        39.0         NaN                  NaN            26.80     NaN   \n",
       "82086        35.2       285.0                 4.64            24.50   143.0   \n",
       "82088        37.6       277.0                 3.57            34.00   139.0   \n",
       "\n",
       "       ...  Told_HA  Told_stroke  Pregnant  HDL_Cholesterol  Glucose  \\\n",
       "1      ...      2.0          2.0       NaN             1.39    4.646   \n",
       "4      ...      2.0          2.0       NaN             1.08    5.550   \n",
       "6      ...      2.0          2.0       2.0             2.73    4.756   \n",
       "9      ...      2.0          2.0       NaN             1.31    4.989   \n",
       "11     ...      2.0          2.0       NaN             0.98    4.606   \n",
       "...    ...      ...          ...       ...              ...      ...   \n",
       "82082  ...      2.0          2.0       NaN             1.27    8.826   \n",
       "82083  ...      2.0          2.0       NaN             1.32      NaN   \n",
       "82085  ...      2.0          2.0       NaN              NaN      NaN   \n",
       "82086  ...      2.0          2.0       NaN             1.42    5.995   \n",
       "82088  ...      2.0          2.0       NaN             1.24      NaN   \n",
       "\n",
       "       Diastolic_Blood_Pressure  Systolic_Blood_Pressure  Diabetes_Case_I  \\\n",
       "1                          56.0                     98.0                0   \n",
       "4                          82.0                    122.0                0   \n",
       "6                          82.0                    124.0                0   \n",
       "9                          96.0                    142.0                0   \n",
       "11                        100.0                    176.0                0   \n",
       "...                         ...                      ...              ...   \n",
       "82082                      68.0                    138.0                1   \n",
       "82083                      66.0                    168.0                0   \n",
       "82085                       NaN                      NaN                0   \n",
       "82086                      76.0                    112.0                0   \n",
       "82088                      80.0                    138.0                0   \n",
       "\n",
       "       Diabetes_Case_II  CVD  \n",
       "1                     0    0  \n",
       "4                     0    0  \n",
       "6                     0    0  \n",
       "9                     0    0  \n",
       "11                    0    0  \n",
       "...                 ...  ...  \n",
       "82082                 0    0  \n",
       "82083                 0    0  \n",
       "82085                 0    0  \n",
       "82086                 1    0  \n",
       "82088                 0    0  \n",
       "\n",
       "[42655 rows x 51 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "## 3. Model Development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train/Test split\n",
    "\n",
    "The paper do a 80/20 split to train the model, trying to keep the target class proportions of the NHANES population in train and test sets:\n",
    "\n",
    "> Downsampling was used to produce a balanced 80/20 train/test split.\n",
    "\n",
    "First, I created a split based on the interaction of all the targets to keep the balance of the target labels. Afterwards, I created a quick R function to check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df['strata'] = df['Diabetes_Case_I'].astype(str) + '_' + df['Diabetes_Case_II'].astype(str) + '_' + df['CVD'].astype(str)\n",
    "train_data, test_data = train_test_split(df, \n",
    "                                         test_size=0.2, \n",
    "                                         random_state=SEED, \n",
    "                                         stratify=df['strata'])\n",
    "\n",
    "train_data = train_data.drop(columns=['strata'])\n",
    "test_data = test_data.drop(columns=['strata'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_data'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc = round((data['Diabetes_Case_I'].sum()/data.shape[0]),4)*100\n",
    "object_name = [name for name in globals() if globals()[name] is train_data][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In df, percentage of cases of Diabetes_Case_I is 13.43%\n",
      "In train_data, percentage of cases of Diabetes_Case_I is 13.43%\n",
      "In test_data, percentage of cases of Diabetes_Case_I is 13.43%\n",
      "In df, percentage of cases of Diabetes_Case_II is 13.08%\n",
      "In train_data, percentage of cases of Diabetes_Case_II is 13.08%\n",
      "In test_data, percentage of cases of Diabetes_Case_II is 13.08%\n",
      "In df, percentage of cases of CVD is 10.84%\n",
      "In train_data, percentage of cases of CVD is 10.84%\n",
      "In test_data, percentage of cases of CVD is 10.84%\n"
     ]
    }
   ],
   "source": [
    "def percent_target(data, target):\n",
    "    percentage = round((data[target].sum()/data.shape[0]),4)*100\n",
    "    object_name = [name for name in globals() if globals()[name] is data][0]\n",
    "    print(f\"In {object_name}, percentage of cases of {target} is {percentage}%\")\n",
    "\n",
    "# Assuming 'df', 'train_data', and 'test_data' are already defined\n",
    "percent_target(df, \"Diabetes_Case_I\")\n",
    "percent_target(train_data, \"Diabetes_Case_I\")\n",
    "percent_target(test_data, \"Diabetes_Case_I\")\n",
    "\n",
    "percent_target(df, \"Diabetes_Case_II\")\n",
    "percent_target(train_data, \"Diabetes_Case_II\")\n",
    "percent_target(test_data, \"Diabetes_Case_II\")\n",
    "\n",
    "percent_target(df, \"CVD\")\n",
    "percent_target(train_data, \"CVD\")\n",
    "percent_target(test_data, \"CVD\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ML pipeline\n",
    "\n",
    "I will use `caret` random search cross-validation implementation, as it might be the closest library to `sklearn` in python. More [here](https://topepo.github.io/caret/random-hyperparameter-search.html).\n",
    "\n",
    "The models are not very intensive in terms of hypertuning (with the exception of XGBoost that I will probably set a time limit). \n",
    "\n",
    "The information about the hypertunning of the model in the paper is rather confusing:\n",
    "\n",
    "\n",
    "> For each model, a grid-search approach with parallelized performance evaluation for model parameter tuning was used to generate the best model parameters. Next, each of the models underwent a 10-fold cross-validation (10 folds of training and testing with randomized data-split)\n",
    "\n",
    "It doesn't make so much sense to do grid-search on the entire training set before a cross-validation. The reason is that you would basically train the data before the validation set that you are using to check if the model generalize well. And then the whole purpose of the cross-validation is to check model fit outside of the training set - it would lose all purpose. \n",
    "\n",
    "My best guess is that they used `GridSearchCV/RandomSearchCV` from sklearn and they assumed that the grid search go first. \n",
    "\n",
    "In the pipeline I also included encoding and standarization similar to the one done by the authors: \n",
    "\n",
    "> Normalization was performed on the data using the following standardization model: x' = x−x^/σ \n",
    "\n",
    "\n",
    "The ML pipelines will follow the these steps, imitatting the paper:\n",
    "\n",
    "1. Apply preprocessing (encoding and standarization) to the initial data. \n",
    "2. Split data into train (80%) and test (20%) sets.\n",
    "3. Train and tune the 4 standard models using 10-fold CV random grid search on the train set.\n",
    "4. Create the Weighted Ensemble Model (WEM) based on the 4 models and weighted by AUC scores.\n",
    "5. Evaluate the final model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables\n",
    "categorical_vars = [\n",
    "    'Race_ethnicity',\n",
    "    'General_health',\n",
    "    'Health_status',\n",
    "    'Told_High_Cholesterol',\n",
    "    'Household_income',\n",
    "    'Relative_Had_Diabetes'\n",
    "]\n",
    "\n",
    "# Numerical variables\n",
    "numerical_vars = [\n",
    "    'Age',\n",
    "    'Alcohol_consumption',\n",
    "    'Arm_circumference',\n",
    "    'Arm_length',\n",
    "    'Osmolality',\n",
    "    'Blood_urea_nitrogen',\n",
    "    'Body_mass_index',\n",
    "    'Chloride',\n",
    "    'Sodium',\n",
    "    'Gamma_glutamyl_transferase',\n",
    "    'Height',\n",
    "    'LDL_cholesterol',\n",
    "    'Leg_length',\n",
    "    'Lymphocytes',\n",
    "    'Mean_cell_volume',\n",
    "    'Pulse',\n",
    "    'Self_reported_greatest_weight',\n",
    "    'Total_cholesterol',\n",
    "    'Triglycerides',\n",
    "    'Waist_circumference',\n",
    "    'Weight',\n",
    "    'White_blood_cell_count',\n",
    "    'Aspartate_aminotransferase_AST',\n",
    "    'Alcohol_Intake',\n",
    "    'Caffeine_Intake',\n",
    "    'Calcium_Intake',\n",
    "    'Carbohydrate_Intake',\n",
    "    'Fiber_Intake',\n",
    "    'Kcal_Intake',\n",
    "    'Sodium_Intake',\n",
    "    'HDL_Cholesterol',\n",
    "    'Diastolic_Blood_Pressure',\n",
    "    'Systolic_Blood_Pressure'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel computation to speed up modeling\n",
    "cl <- makePSOCKcluster(detectCores() - 1)\n",
    "registerDoParallel(cl)\n",
    "\n",
    "#target_vars <- c(\"Diabetes_Case_I\", \"Diabetes_Case_II\", \"CVD\")\n",
    "data <- train_data\n",
    "target <- \"Diabetes_Case_I\"\n",
    "\n",
    "# Create Target \n",
    "y <-  as.factor(data[[target]])\n",
    "\n",
    "# Standarization and encoding of categorical variables.\n",
    "X <- data |> select(all_of(c(numerical_vars, categorical_vars)))\n",
    "\n",
    "# Preprocessing\n",
    "rec <- recipe(~ ., data = X) |>\n",
    "  step_unknown(all_of(categorical_vars)) |> # assign a missing value in a factor level to \"unknown\".\n",
    "  step_mutate_at(all_of(categorical_vars), fn = as.factor) |>\n",
    "  step_integer(all_of(categorical_vars)) |>\n",
    "  step_normalize(all_of(numerical_vars)) |>\n",
    "  step_impute_mean(all_of(numerical_vars)) # Needed for lr and svm\n",
    "\n",
    "prep_rec <- prep(rec)\n",
    "X_processed <- bake(prep_rec, new_data = X)\n",
    "X_processed <- as.data.frame(X_processed)\n",
    "\n",
    "# 10F CV with Random Search\n",
    "control_cv <- trainControl(\n",
    "    method = \"cv\",\n",
    "    number = 10,\n",
    "    search = \"random\",\n",
    "    classProbs = TRUE,\n",
    "    summaryFunction = twoClassSummary,\n",
    "  )\n",
    "\n",
    "# Models \n",
    "set.seed(SEED)\n",
    "\n",
    "# Model 1 : Logistic Regression\n",
    "lr <- train(x = X_processed,\n",
    "            y = y,\n",
    "            method=\"glm\",\n",
    "            family=\"binomial\",\n",
    "            metric=\"ROC\",\n",
    "            trControl=control_cv\n",
    "                      )\n",
    "\n",
    "# Model 2: Support Vector Machine\n",
    "svm <- train(x = X_processed,\n",
    "            y = y,\n",
    "            method=\"svmRadialWeights\",\n",
    "            metric=\"ROC\",\n",
    "            trControl=control_cv,\n",
    "            tuneLength=10\n",
    "     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "stopCluster(cl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: XGBoost \n",
    "\n",
    "tune_grid <- expand.grid(nrounds = 200,\n",
    "                        max_depth = 5,\n",
    "                        eta = 0.05,\n",
    "                        gamma = 0.01,\n",
    "                        colsample_bytree = 0.75,\n",
    "                        min_child_weight = 0,\n",
    "                        subsample = 0.5)\n",
    "\n",
    "xgb <- train(x = X_processed,\n",
    "            y = y,\n",
    "            method=\"svmRadial\",\n",
    "            metric=\"ROC\",\n",
    "            trControl=control_cv,\n",
    "            tuneGrid = tune_grid,\n",
    "            tuneLength = 10)\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "### 3.3 Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models used in the paper:\n",
    "\n",
    "- Logistic Regression\n",
    "- Support Vector Machine\n",
    "- Random Forest\n",
    "- Gradient Boosted Trees\n",
    "- Ensemble model of the 5 models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(SEED)\n",
    "\n",
    "# Model 1 : Logistic Regression\n",
    "lr <- train(x = X_train,\n",
    "            y = y_train,\n",
    "            method=\"glm\",\n",
    "            family=\"binomial\",\n",
    "            metric=\"AUC\",\n",
    "            trControl=control_cv,\n",
    "            tuneLength=1\n",
    "                      )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_auc <- function(model, X_data, y_data) {\n",
    "\n",
    "    # AUC \n",
    "    predictions <- predict(model, newdata = X_data, type = \"prob\")\n",
    "    roc_score <- roc(y_data, predictions[, \"Yes\"], quiet = TRUE)\n",
    "    auc_score <- round(auc(roc_score), 3)\n",
    "    # Message \n",
    "    data_name <- deparse(substitute(X_data))\n",
    "    glue(\"AUC score for Logistic Regression on {data_name}: {auc_score} \")\n",
    "\n",
    "}\n",
    "\n",
    "model_auc(lr, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "\n",
    "# Convert ROC object to data frame\n",
    "roc_data <- data.frame(\n",
    "  FPR = 1 - roc_obj$specificities,\n",
    "  TPR = roc_obj$sensitivities\n",
    ")\n",
    "\n",
    "ggplot(roc_data, aes(x = FPR, y = TPR)) +\n",
    "  geom_line(color = \"blue\", size = 1) +\n",
    "  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"red\") +\n",
    "  labs(title = \"ROC Curve\",\n",
    "       x = \"False Positive Rate\", \n",
    "       y = \"True Positive Rate\") +\n",
    "  annotate(\"text\", x = 0.75, y = 0.25, \n",
    "           label = paste(\"AUC =\", round(auc(roc_obj), 3))) +\n",
    "  theme_minimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter nbconvert --to markdown R_replicate_Dinh.ipynb --output README.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
